#!/usr/bin/env python3
"""
DataFlow Integration Module
============================

This module provides integration with the OpenDCAI DataFlow system.
It includes utilities for text processing, reasoning enhancement, and knowledge base cleaning.
"""

import sys
import os
import logging
from pathlib import Path

# æ·»åŠ  DataFlow åˆ° Python è·¯å¾„
PROJECT_ROOT = Path(__file__).parent
DATAFLOW_PATH = PROJECT_ROOT / "Dataflow"

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class DataFlowIntegration:
    """DataFlow é›†æˆç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ– DataFlow é›†æˆ"""
        self.dataflow_available = False
        self.dataflow_path_exists = DATAFLOW_PATH.exists()
        self.error_message = None
        self._init_dataflow()
    
    def _init_dataflow(self):
        """åˆå§‹åŒ– DataFlow æ¨¡å—"""
        try:
            # æ£€æŸ¥DataFlowç›®å½•æ˜¯å¦å­˜åœ¨
            if not self.dataflow_path_exists:
                self.error_message = f"DataFlowç›®å½•ä¸å­˜åœ¨: {DATAFLOW_PATH}"
                logger.warning(self.error_message)
                return
            
            # æ·»åŠ DataFlowè·¯å¾„åˆ°sys.path
            if str(DATAFLOW_PATH) not in sys.path:
                sys.path.insert(0, str(DATAFLOW_PATH))
            
            # å°è¯•å¯¼å…¥ DataFlow çœŸå®æ¨¡å—
            try:
                # å¯¼å…¥é¢„è®­ç»ƒè¿‡æ»¤æµæ°´çº¿
                from dataflow.statics.pipelines.cpu_pipelines.text_pt_filter import PTTextPipeline
                from dataflow.statics.pipelines.cpu_pipelines.text_sft_filter import SFTTextPipeline
                from dataflow.utils.storage import FileStorage
                
                # å­˜å‚¨çœŸå®çš„pipelineç±»
                self.PTTextPipeline = PTTextPipeline
                self.SFTTextPipeline = SFTTextPipeline
                self.ChineseTextPipeline = ChineseTextPipeline  # æ·»åŠ ä¸­æ–‡å‹å¥½çš„pipeline
                self.FileStorage = FileStorage
                self.dataflow_available = True
                
                logger.info("DataFlow æ¨¡å—åŠ è½½æˆåŠŸ")
                
            except ImportError as e:
                logger.warning(f"DataFlow æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
                self.dataflow_available = False
                self.error_message = f"DataFlowæ¨¡å—å¯¼å…¥å¤±è´¥: {e}"
                logger.warning("DataFlowåŠŸèƒ½å°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
                
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–DataFlowå¤±è´¥: {e}")
            self.dataflow_available = False
            self.error_message = f"DataFlowåˆå§‹åŒ–å¤±è´¥: {e}"
    
    def is_available(self):
        """æ£€æŸ¥ DataFlow æ˜¯å¦å¯ç”¨"""
        return self.dataflow_available
    
    def create_pretrain_filter_pipeline(self, config=None):
        """åˆ›å»ºé¢„è®­ç»ƒæ•°æ®è¿‡æ»¤ç®¡é“"""
        if not self.dataflow_available:
            logger.info("DataFlowä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼åˆ›å»ºé¢„è®­ç»ƒæ•°æ®è¿‡æ»¤ç®¡é“")
            return MockPipeline("pretrain_filter", config)
        
        try:
            # å¦‚æœDataFlowå®é™…å¯ç”¨ï¼Œä¼˜å…ˆä½¿ç”¨ä¸­æ–‡å‹å¥½çš„pipeline
            if hasattr(self, 'ChineseTextPipeline'):
                logger.info("ä½¿ç”¨ä¸­æ–‡å‹å¥½çš„DataFlowç®¡é“")
                return DataFlowPipelineWrapper(self.ChineseTextPipeline, "chinese_pretrain_filter")
            elif hasattr(self, 'PTTextPipeline'):
                logger.info("ä½¿ç”¨æ ‡å‡†DataFlowé¢„è®­ç»ƒè¿‡æ»¤ç®¡é“")
                return DataFlowPipelineWrapper(self.PTTextPipeline, "pretrain_filter")
            else:
                # ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼
                logger.info("ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼åˆ›å»ºé¢„è®­ç»ƒæ•°æ®è¿‡æ»¤ç®¡é“")
                return MockPipeline("pretrain_filter", config)
            
        except Exception as e:
            logger.error(f"åˆ›å»ºé¢„è®­ç»ƒæ•°æ®è¿‡æ»¤ç®¡é“å¤±è´¥: {e}")
            # è¿”å›æ¨¡æ‹Ÿç®¡é“è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            return MockPipeline("pretrain_filter", config)
    
    def create_sft_filter_pipeline(self, config=None):
        """åˆ›å»ºSFTæ•°æ®è¿‡æ»¤ç®¡é“"""
        if not self.dataflow_available:
            logger.info("DataFlowä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼åˆ›å»ºSFTæ•°æ®è¿‡æ»¤ç®¡é“")
            return MockPipeline("sft_filter", config)
        
        try:
            # å¦‚æœDataFlowå®é™…å¯ç”¨ï¼Œä½¿ç”¨çœŸå®çš„pipeline
            if hasattr(self, 'SFTTextPipeline'):
                logger.info("ä½¿ç”¨DataFlowåˆ›å»ºSFTæ•°æ®è¿‡æ»¤ç®¡é“")
                return DataFlowPipelineWrapper(self.SFTTextPipeline, "sft_filter")
            else:
                # ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼
                logger.info("ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼åˆ›å»ºSFTæ•°æ®è¿‡æ»¤ç®¡é“")
                return MockPipeline("sft_filter", config)
            
        except Exception as e:
            logger.error(f"åˆ›å»ºSFTæ•°æ®è¿‡æ»¤ç®¡é“å¤±è´¥: {e}")
            # è¿”å›æ¨¡æ‹Ÿç®¡é“è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            return MockPipeline("sft_filter", config)
    
    def create_pretrain_synthetic_pipeline(self, config=None):
        """åˆ›å»ºé¢„è®­ç»ƒæ•°æ®åˆæˆç®¡é“ï¼ˆç±»phi-4ï¼‰"""
        if not self.dataflow_available:
            raise RuntimeError("DataFlow æœªæ­£ç¡®åŠ è½½")
        
        try:
            config = config or {
                "model_name": "Qwen/Qwen2.5-7B-Instruct",
                "max_tokens": 8192,
                "temperature": 0.7,
                "qa_pairs_per_chunk": 3,
                "chunk_size": 1000,
                "chunk_overlap": 200
            }
            
            if hasattr(self, 'dataflow'):
                pipeline = self.dataflow.create_pipeline("pretrain_synthetic", config)
                logger.info("é¢„è®­ç»ƒæ•°æ®åˆæˆç®¡é“åˆ›å»ºæˆåŠŸ")
                return pipeline
            else:
                logger.info("ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼åˆ›å»ºé¢„è®­ç»ƒæ•°æ®åˆæˆç®¡é“")
                return MockPipeline("pretrain_synthetic", config)
            
        except Exception as e:
            logger.error(f"åˆ›å»ºé¢„è®­ç»ƒæ•°æ®åˆæˆç®¡é“å¤±è´¥: {e}")
            return MockPipeline("pretrain_synthetic", config)
    
    def create_reasoning_pipeline(self, config=None):
        """åˆ›å»ºæ¨ç†å¢å¼ºç®¡é“"""
        if not self.dataflow_available:
            raise RuntimeError("DataFlow æœªæ­£ç¡®åŠ è½½")
        
        try:
            config = config or {
                "model_name": "gpt-4",
                "chain_of_thought": True,
                "difficulty_estimation": True
            }
            
            if hasattr(self, 'dataflow'):
                pipeline = self.dataflow.create_pipeline("reasoning", config)
                logger.info("æ¨ç†å¢å¼ºç®¡é“åˆ›å»ºæˆåŠŸ")
                return pipeline
            else:
                logger.info("ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼åˆ›å»ºæ¨ç†å¢å¼ºç®¡é“")
                return MockPipeline("reasoning", config)
            
        except Exception as e:
            logger.error(f"åˆ›å»ºæ¨ç†å¢å¼ºç®¡é“å¤±è´¥: {e}")
            return MockPipeline("reasoning", config)
    
    def create_knowledge_base_pipeline(self, config=None):
        """åˆ›å»ºçŸ¥è¯†åº“æ¸…ç†ç®¡é“"""
        if not self.dataflow_available:
            raise RuntimeError("DataFlow æœªæ­£ç¡®åŠ è½½")
        
        try:
            config = config or {
                "chunk_size": 1000,
                "chunk_overlap": 200,
                "quality_threshold": 0.8
            }
            
            if hasattr(self, 'dataflow'):
                pipeline = self.dataflow.create_pipeline("knowledge_base", config)
                logger.info("çŸ¥è¯†åº“æ¸…ç†ç®¡é“åˆ›å»ºæˆåŠŸ")
                return pipeline
            else:
                logger.info("ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼åˆ›å»ºçŸ¥è¯†åº“æ¸…ç†ç®¡é“")
                return MockPipeline("knowledge_base", config)
            
        except Exception as e:
            logger.error(f"åˆ›å»ºçŸ¥è¯†åº“æ¸…ç†ç®¡é“å¤±è´¥: {e}")
            return MockPipeline("knowledge_base", config)
    
    def process_text_data(self, text_data, pipeline_type="pretrain_filter", config=None):
        """å¤„ç†æ–‡æœ¬æ•°æ®"""
        if not self.dataflow_available:
            logger.warning("DataFlow ä¸å¯ç”¨ï¼Œè¿”å›åŸå§‹æ•°æ®")
            return text_data
        
        try:
            if pipeline_type == "pretrain_filter":
                pipeline = self.create_pretrain_filter_pipeline(config)
            elif pipeline_type == "sft_filter":
                pipeline = self.create_sft_filter_pipeline(config)
            elif pipeline_type == "pretrain_synthetic":
                pipeline = self.create_pretrain_synthetic_pipeline(config)
            elif pipeline_type == "reasoning":
                pipeline = self.create_reasoning_pipeline(config)
            elif pipeline_type == "knowledge_base":
                pipeline = self.create_knowledge_base_pipeline(config)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„ç®¡é“ç±»å‹: {pipeline_type}")
            
            # å¤„ç†æ•°æ®
            processed_data = pipeline.process(text_data)
            logger.info(f"ä½¿ç”¨ {pipeline_type} ç®¡é“å¤„ç†æ•°æ®å®Œæˆ")
            return processed_data
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡æœ¬æ•°æ®å¤±è´¥: {e}")
            return text_data
    
    def process_markdown_files(self, file_paths, pipeline_type="pretrain_filter", config=None):
        """æ‰¹é‡å¤„ç†Markdownæ–‡ä»¶"""
        if not self.dataflow_available:
            logger.warning("DataFlow ä¸å¯ç”¨ï¼Œè·³è¿‡å¤„ç†")
            return []
        
        results = []
        try:
            for file_path in file_paths:
                try:
                    # è¯»å–Markdownæ–‡ä»¶
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # å¤„ç†å†…å®¹
                    processed_content = self.process_text_data(content, pipeline_type, config)
                    
                    results.append({
                        'file_path': file_path,
                        'original_content': content,
                        'processed_content': processed_content,
                        'status': 'success'
                    })
                    
                except Exception as e:
                    logger.error(f"å¤„ç†æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
                    results.append({
                        'file_path': file_path,
                        'status': 'failed',
                        'error': str(e)
                    })
            
            return results
            
        except Exception as e:
            logger.error(f"æ‰¹é‡å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
            return results
    
    def health_check(self):
        """å¥åº·æ£€æŸ¥"""
        status = {
            "dataflow_available": self.dataflow_available,
            "dataflow_path": str(DATAFLOW_PATH),
            "dataflow_exists": self.dataflow_path_exists,
            "version": "1.0.0"
        }
        
        if self.error_message:
            status["error_message"] = self.error_message
        
        if self.dataflow_available and hasattr(self, 'dataflow'):
            try:
                # æ£€æŸ¥æ ¸å¿ƒæ¨¡å—
                from dataflow import __version__
                status["dataflow_version"] = __version__
            except:
                status["dataflow_version"] = "unknown"
        else:
            status["dataflow_version"] = "mock_mode"
        
        return status


class DataFlowPipelineWrapper:
    """DataFlowç®¡é“å°è£…å™¨ï¼Œç”¨äºé€‚é…æˆ‘ä»¬çš„æ¥å£"""
    
    def __init__(self, pipeline_class, pipeline_type):
        self.pipeline_class = pipeline_class
        self.pipeline_type = pipeline_type
        logger.info(f"åˆ›å»ºDataFlowç®¡é“: {pipeline_type}")
    
    def process(self, text_data):
        """å¤„ç†æ•°æ®ï¼Œé€‚é…DataFlowçš„æ¥å£"""
        import tempfile
        import json
        import os
        
        try:
            # åˆ›å»ºä¸´æ—¶ç›®å½•
            with tempfile.TemporaryDirectory() as temp_dir:
                input_file = os.path.join(temp_dir, "input.jsonl")
                cache_dir = os.path.join(temp_dir, "cache")
                os.makedirs(cache_dir, exist_ok=True)
                
                # å‡†å¤‡è¾“å…¥æ•°æ®
                input_data = {"raw_content": text_data}
                with open(input_file, 'w', encoding='utf-8') as f:
                    json.dump(input_data, f, ensure_ascii=False)
                    f.write('\n')
                
                # åˆ›å»ºè‡ªå®šä¹‰çš„pipelineå®ä¾‹
                pipeline = self.pipeline_class()
                # ä¿®æ”¹storageé…ç½®ä»¥ä½¿ç”¨æˆ‘ä»¬çš„ä¸´æ—¶æ–‡ä»¶
                from dataflow.utils.storage import FileStorage
                pipeline.storage = FileStorage(
                    first_entry_file_name=input_file,
                    cache_path=cache_dir,
                    file_name_prefix="dataflow_cache_step",
                    cache_type="jsonl",
                )
                
                # æ‰§è¡Œpipeline
                logger.info(f"å¼€å§‹æ‰§è¡ŒDataFlowç®¡é“: {self.pipeline_type}")
                pipeline.forward()
                
                # è·å–æœ€ç»ˆç»“æœ
                result_files = [f for f in os.listdir(cache_dir) if f.endswith('.jsonl')]
                if result_files:
                    # ä½¿ç”¨æœ€æ–°çš„æ–‡ä»¶
                    latest_file = max(result_files, key=lambda x: os.path.getmtime(os.path.join(cache_dir, x)))
                    result_file = os.path.join(cache_dir, latest_file)
                    
                    # è¯»å–ç»“æœ
                    processed_data = []
                    with open(result_file, 'r', encoding='utf-8') as f:
                        for line in f:
                            if line.strip():
                                try:
                                    data = json.loads(line)
                                    processed_data.append(data)
                                except json.JSONDecodeError:
                                    continue
                    
                    if processed_data:
                        logger.info(f"DataFlowç®¡é“å¤„ç†å®Œæˆï¼Œå¤„ç†äº† {len(processed_data)} æ¡æ•°æ®")
                        return processed_data
                    else:
                        logger.warning("DataFlowç®¡é“å¤„ç†åæ— æ•°æ®ä¿ç•™ï¼Œå¯èƒ½è¢«è¿‡æ»¤å™¨è¿‡æ»¤æ‰äº†")
                        # è¿”å›åŒ…å«åŸå§‹æ•°æ®çš„ç»“æœï¼Œæ ‡è®°ä¸ºè¢«è¿‡æ»¤
                        return [{
                            "raw_content": text_data,
                            "filtered": True,
                            "reason": "æ•°æ®è¢«DataFlowè¿‡æ»¤å™¨è¿‡æ»¤æ‰"
                        }]
                else:
                    logger.warning("DataFlowç®¡é“æœªç”Ÿæˆç»“æœæ–‡ä»¶")
                    return [{
                        "raw_content": text_data,
                        "filtered": False,
                        "reason": "DataFlowç®¡é“æœªç”Ÿæˆç»“æœæ–‡ä»¶"
                    }]
                
        except Exception as e:
            logger.error(f"DataFlowç®¡é“å¤„ç†å¤±è´¥: {e}")
            # è¿”å›åŒ…å«åŸå§‹æ•°æ®çš„ç»“æœï¼Œæ ‡è®°ä¸ºå¤„ç†å¤±è´¥
            return [{
                "raw_content": text_data,
                "error": True,
                "reason": f"DataFlowå¤„ç†å¤±è´¥: {str(e)}"
            }]


class MockPipeline:
    """æ¨¡æ‹ŸDataFlowç®¡é“ï¼Œç”¨äºæµ‹è¯•å’Œå¼€å‘"""
    
    def __init__(self, pipeline_type, config):
        self.pipeline_type = pipeline_type
        self.config = config
        logger.info(f"åˆ›å»ºæ¨¡æ‹Ÿç®¡é“: {pipeline_type}")
    
    def process(self, text_data):
        """æ¨¡æ‹Ÿå¤„ç†æ•°æ®"""
        logger.info(f"ä½¿ç”¨æ¨¡æ‹Ÿç®¡é“ {self.pipeline_type} å¤„ç†æ•°æ®")
        
        # æ ¹æ®ç®¡é“ç±»å‹è¿”å›ä¸åŒçš„æ¨¡æ‹Ÿç»“æœ
        if self.pipeline_type == "pretrain_filter":
            return f"[é¢„è®­ç»ƒè¿‡æ»¤] {text_data}"
        elif self.pipeline_type == "pretrain_synthetic":
            return f"[é¢„è®­ç»ƒåˆæˆ] åŸºäºè¾“å…¥: {text_data[:100]}... ç”Ÿæˆçš„åˆæˆæ•°æ®"
        elif self.pipeline_type == "reasoning":
            return f"[æ¨ç†å¢å¼º] {text_data}"
        elif self.pipeline_type == "knowledge_base":
            return f"[çŸ¥è¯†åº“æ¸…ç†] {text_data}"
        else:
            return text_data


class ChineseTextPipeline:
    """ä¸­æ–‡å‹å¥½çš„DataFlowç®¡é“"""
    
    def __init__(self):
        self.storage = None  # å°†åœ¨è¿è¡Œæ—¶è®¾ç½®
        self._init_operators()
    
    def _init_operators(self):
        """åˆå§‹åŒ–æ“ä½œç¬¦ï¼Œä½¿ç”¨å¯¹ä¸­æ–‡å‹å¥½çš„å‚æ•°"""
        try:
            from dataflow.operators.refine.GeneralText import (
                RemoveExtraSpacesRefiner,
                RemoveEmojiRefiner,
                HtmlUrlRemoverRefiner
            )
            from dataflow.operators.process.GeneralText import (
                ContentNullFilter,
                CharNumberFilter,
                SentenceNumberFilter,
                WatermarkFilter
            )
            
            # ä½¿ç”¨åŸºæœ¬çš„refineæ“ä½œç¬¦
            self.remove_extra_spaces_refiner = RemoveExtraSpacesRefiner()
            self.remove_emoji_refiner = RemoveEmojiRefiner()
            self.html_remove_refiner = HtmlUrlRemoverRefiner()
            
            # ä½¿ç”¨å¯¹ä¸­æ–‡å‹å¥½çš„è¿‡æ»¤å™¨
            self.content_null_filter = ContentNullFilter()
            self.char_number_filter = CharNumberFilter(threshold=50)  # é™ä½å­—ç¬¦æ•°è¦æ±‚
            self.sentence_number_filter = SentenceNumberFilter(min_sentences=1, max_sentences=10000)  # æ›´å®½æ¾çš„å¥å­æ•°è¦æ±‚
            self.watermark_filter = WatermarkFilter(watermarks=['Copyright', 'Watermark', 'Confidential', 'ç‰ˆæƒ', 'æ°´å°'])
            
            logger.info("ä¸­æ–‡å‹å¥½ç®¡é“æ“ä½œç¬¦åˆå§‹åŒ–æˆåŠŸ")
            
        except ImportError as e:
            logger.error(f"æ— æ³•å¯¼å…¥DataFlowæ“ä½œç¬¦: {e}")
            raise
    
    def forward(self):
        """æ‰§è¡Œç®¡é“å¤„ç†"""
        try:
            # æ¸…ç†å’Œé¢„å¤„ç†
            self.remove_extra_spaces_refiner.run(
                storage=self.storage.step(),
                input_key="raw_content"
            )
            self.remove_emoji_refiner.run(
                storage=self.storage.step(),
                input_key="raw_content"
            )
            self.html_remove_refiner.run(
                storage=self.storage.step(),
                input_key="raw_content"
            )
            
            # åŸºæœ¬è´¨é‡è¿‡æ»¤
            self.content_null_filter.run(
                storage=self.storage.step(),
                input_key='raw_content',
            )
            self.char_number_filter.run(
                storage=self.storage.step(),
                input_key='raw_content',
            )
            self.sentence_number_filter.run(
                storage=self.storage.step(),
                input_key='raw_content'
            )
            self.watermark_filter.run(
                storage=self.storage.step(),
                input_key='raw_content',
            )
            
            logger.info("ä¸­æ–‡å‹å¥½ç®¡é“å¤„ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"ä¸­æ–‡å‹å¥½ç®¡é“å¤„ç†å¤±è´¥: {e}")
            raise


# åˆ›å»ºå…¨å±€å®ä¾‹
dataflow_integration = DataFlowIntegration()

# ä¾¿æ·å‡½æ•°
def process_text(text, pipeline_type="pretrain_filter", config=None):
    """å¤„ç†æ–‡æœ¬çš„ä¾¿æ·å‡½æ•°"""
    return dataflow_integration.process_text_data(text, pipeline_type, config)

def filter_pretrain_data(text, config=None):
    """é¢„è®­ç»ƒæ•°æ®è¿‡æ»¤"""
    return dataflow_integration.process_text_data(text, "pretrain_filter", config)

def generate_pretrain_data(text, config=None):
    """ç”Ÿæˆé¢„è®­ç»ƒæ•°æ®ï¼ˆç±»phi-4æ ¼å¼ï¼‰"""
    return dataflow_integration.process_text_data(text, "pretrain_synthetic", config)

def process_library_markdown_files(file_paths, pipeline_type="pretrain_filter", config=None):
    """å¤„ç†æ–‡ä»¶åº“çš„Markdownæ–‡ä»¶"""
    return dataflow_integration.process_markdown_files(file_paths, pipeline_type, config)

def enhance_reasoning(qa_pairs):
    """å¢å¼ºæ¨ç†é“¾"""
    return dataflow_integration.process_text_data(qa_pairs, "reasoning")

def clean_knowledge_base(raw_data):
    """æ¸…ç†çŸ¥è¯†åº“æ•°æ®"""
    return dataflow_integration.process_text_data(raw_data, "knowledge_base")

def health_check():
    """å¥åº·æ£€æŸ¥"""
    return dataflow_integration.health_check()

# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    print("ğŸš€ DataFlow é›†æˆæµ‹è¯•")
    print("=" * 50)
    
    # å¥åº·æ£€æŸ¥
    status = health_check()
    print(f"DataFlow çŠ¶æ€: {status}")
    
    if dataflow_integration.is_available():
        print("âœ… DataFlow å¯ç”¨ï¼Œå¯ä»¥å¼€å§‹å¤„ç†æ•°æ®")
        
        # ç¤ºä¾‹æ–‡æœ¬å¤„ç†
        sample_text = "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚"
        
        try:
            processed = process_text(sample_text)
            print(f"å¤„ç†ç»“æœ: {processed}")
        except Exception as e:
            print(f"å¤„ç†å¤±è´¥: {e}")
    else:
        print("âŒ DataFlow ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥å®‰è£…") 