

<div align="center">

# ğŸ”® PinData

[![English](https://img.shields.io/badge/Lang-English-blue)](README.md) | [![ä¸­æ–‡](https://img.shields.io/badge/Lang-ä¸­æ–‡-red)](README_CN.md)

*Enterprise Data Management Platform for the AI Era*

![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)
![Python](https://img.shields.io/badge/python-3.8+-green.svg)
![React](https://img.shields.io/badge/react-18+-61dafb.svg)
![Docker](https://img.shields.io/badge/docker-ready-blue.svg)

[ğŸš€ Quick Start](#-quick-start) â€¢
[ğŸ“– Documentation](#-documentation) â€¢
[ğŸ¯ Use Cases](#-use-cases) â€¢
[ğŸ—ï¸ Architecture](#ï¸-architecture) â€¢
[ğŸ¤ Contributing](#-contributing)

</div>

---

## âœ¨ What is PinData?


![PinData Poster](./poster.png)

PinData is a **modern enterprise data management platform** for the AI era. It combines data engineering and dataset management capabilities to unify processing and analysis of both structured and unstructured enterprise data, transforming raw data into structured knowledge assets and high-quality training datasets that provide a solid data foundation for enterprise AI applications.

### ğŸ¯ Why PinData?

- **ğŸ”§ Data Engineering Capabilities**: Unified processing of structured and unstructured data with complete data pipelines
- **ğŸ§  Knowledge Asset Creation**: Transform enterprise raw data into structured knowledge repositories, enhancing data value
- **ğŸ“š Multi-Format Compatibility**: Intelligent processing of DOCX, PPTX, PDF and various enterprise document formats
- **ğŸ¤– AI-Driven Analysis**: Integrated large language models for intelligent data extraction and structured transformation
- **ğŸ“Š Versioned Management**: Git-style data lineage tracking ensuring data governance and compliance
- **ğŸ”— Ecosystem Integration**: Seamless integration with mainstream data platforms and AI training frameworks
- **ğŸš€ Enterprise Architecture**: Built on modern technology stack supporting large-scale deployment and scaling

---

## ğŸ¯ Use Cases

### ğŸ¢ Large Enterprise Data Management
- **Knowledge Asset Inventory**: Unify and structure scattered enterprise documents, reports, and manuals
- **Data Governance & Compliance**: Establish complete data lineage tracking to meet regulatory and audit requirements
- **Cross-Department Collaboration**: Unified data management platform breaking down data silos and promoting knowledge sharing

### ğŸ¤– Enterprise AI Transformation
- **Intelligent Knowledge Base**: Transform enterprise knowledge into structured data suitable for AI model learning
- **Business Data Mining**: Extract key information and insights from unstructured business documents
- **Customized AI Training**: Build high-quality training datasets for enterprise-specific scenarios

### ğŸ“ Academic & Research Institutions
- **Literature Management**: Large-scale processing and analysis of academic literature, building research databases
- **Interdisciplinary Research**: Unified management of multi-domain, multi-format research materials
- **Knowledge Graph Construction**: Transform research outcomes into structured knowledge networks (planned)

### ğŸ¥ Professional Service Organizations
- **Case Library Management**: Transform historical cases and reports into analyzable structured data
- **Professional Knowledge Transfer**: Systematically store and transfer expert experience and knowledge
- **Business Intelligence Analysis**: Extract trends and patterns from business documents to support decision-making

---

## ğŸ—ï¸ Architecture

```mermaid
graph TD
    A[ğŸ“„ Unstructured Data] --> B[ğŸ”§ Data Engineering Layer]
    A1[ğŸ“Š Structured Data] --> B
    B --> C[ğŸ“ Unified Parser]
    C --> D[ğŸ§¹ Data Cleaner]
    D --> E[ğŸ” Smart Extractor]
    E --> F[ğŸ§  Knowledge Structuring]
    F --> G[ğŸ“Š Dataset Generation]
    F --> H[ğŸ’¾ Knowledge Base]
    
    I[ğŸ›ï¸ Enterprise Interface] --> J[ğŸš€ API Gateway]
    J --> K[ğŸ—„ï¸ Metadata DB]
    J --> L[ğŸ“¦ Object Storage]
    J --> M[â° Task Scheduler]
    J --> N[ğŸ¤– AI Service Layer]
    
    style A fill:#e1f5fe
    style A1 fill:#e1f5fe
    style G fill:#e8f5e8
    style H fill:#f3e5f5
    style I fill:#fff3e0
```

### Core Components

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Data Engineering Layer** | Plugin-based processor architecture | Unified processing of structured and unstructured data |
| **Intelligent Parsing Engine** | MarkItDown + Custom parsers | Multi-format document parsing and content extraction |
| **Knowledge Structuring Service** | LangChain + Multi-LLM support | AI-driven content analysis and knowledge extraction |
| **Enterprise Interface** | React 18 + TypeScript | Management console for enterprise users |
| **Task Scheduling Engine** | Celery + Redis | Scheduling and management of data processing tasks |
| **Hybrid Storage System** | MinIO + PostgreSQL | File object storage + relational metadata management |

---

## ğŸš€ Quick Start

### Prerequisites
- ğŸ³ Docker Engine 20.10+ (The `docker compose` command is included automatically)
- ğŸ’¾ 4GB+ available RAM
- ğŸ“ 2GB+ available disk space

### One-Command Setup

```bash
# Clone the repository
git clone https://github.com/zutoData/pindata.git
cd pindata

# Start all services
docker compose up -d

# Access the application
open http://localhost:3000
```

### Build Your Enterprise Knowledge Assets in 3 Steps

1. **ğŸ“ Import Data**: Upload various enterprise documents and data files with unified management for multiple formats
2. **ğŸ”§ Intelligent Processing**: Automatically parse, clean, and structure your data through data engineering pipelines
3. **ğŸ§  Knowledge Transformation**: Use AI technology to transform data into structured knowledge bases and usable datasets

---

## ğŸ’¡ Key Features

### ğŸ”§ Unified Data Engineering Pipeline
- **Multi-Source Data Integration**: Unified processing of structured and unstructured data
- **Intelligent Data Cleaning**: Automatically identify and handle data quality issues
- **Flexible Data Transformation**: Configurable data processing and transformation rules
- **Batch Parallel Processing**: Efficiently process large-scale datasets

### ğŸ§  AI-Driven Knowledge Extraction
- **Intelligent Content Analysis**: Use large language models for deep understanding and analysis of data content
- **Automatic Knowledge Extraction**: Automatically extract structured knowledge from unstructured data
- **Multi-LLM Support**: Support for OpenAI, Google Gemini, Anthropic Claude and other mainstream models
- **Customizable Extraction Rules**: Customize knowledge extraction strategies according to business needs

### ğŸ“Š Enterprise-Grade Data Management
- **Versioned Management**: Git-style data lineage tracking and version control
- **Permissions & Security**: Fine-grained data access control and security management
- **Data Governance**: Complete data lifecycle management and compliance support
- **Multi-Platform Integration**: Seamless integration with mainstream data platforms and AI training frameworks

### ğŸ”Œ Extensible Plugin Architecture (Planned)
```python
# Create custom data processors
class CustomDataProcessor(BaseProcessor):
    def process(self, data, config):
        # Your custom data processing logic
        return processed_data
        
# Create custom knowledge extractors  
class CustomKnowledgeExtractor(BaseExtractor):
    def extract(self, content, schema):
        # Your custom knowledge extraction logic
        return extracted_knowledge
```

---

## ğŸ› ï¸ Development

### Local Development Setup

```bash
# Backend development
cd backend
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
cp config.example.env .env
python run.py

# Frontend development (new terminal)
cd frontend
npm install
npm run dev

# Celery worker (new terminal)
cd backend
./start_celery.sh
```

### Current Development Focus

The project is actively developing core features:
- Unified data engineering pipeline and multi-source data integration
- AI-driven intelligent data analysis and knowledge extraction
- Enterprise-grade data governance and permission management
- Extensible plugin system and custom processors
- Multi-platform data source integration and API interfaces

---

## ğŸ“Š Roadmap

### ğŸ¯ Current Version (0.0.4) - Basic Data Engineering Platform
- âœ… Multi-format unified data integration and parsing
- âœ… AI-driven intelligent data analysis and knowledge extraction
- âœ… Data versioning and lineage tracking
- âœ… Enterprise-grade management interface
- âœ… Plugin-based data processing architecture

### ğŸš§ Next Release (1.0) - Enterprise Enhancement
- ğŸ”„ Advanced data governance and compliance management
- ğŸ“Š Enterprise data asset inventory and analytics dashboard
- ğŸ‘¥ Multi-user collaboration and permission management system
- ğŸ”Œ Rich data source connectors (databases, APIs, file systems)
- ğŸ“ˆ Data quality monitoring and anomaly detection

### ğŸ¢ Enterprise Version (2.0) - Comprehensive Data Management
- ğŸ—ƒï¸ Enterprise-grade data warehouse integration
- ğŸ¤– Automated data pipelines and workflows
- ğŸ“‹ Data catalog and metadata management
- ğŸ” Intelligent data discovery and recommendations
- ğŸ“Š Advanced business intelligence and reporting features

### ğŸŒŸ Future Vision - AI-Native Data Platform
- ğŸ–¼ï¸ Multi-modal data processing (images, audio, video)
- ğŸ§  Self-learning data processing and optimization
- â˜ï¸ Hybrid cloud and multi-cloud deployment support
- ğŸŒ Real-time data streaming and analysis
- ğŸ¤– AI Agent-driven automated data management

---

## ğŸ¤ Contributing

We welcome contributions from the community! Whether you're fixing bugs, adding features, or improving documentation, your help makes PinData better for everyone.

### Ways to Contribute
- ğŸ› **Bug Reports**: Found an issue? Let us know!
- âœ¨ **Feature Requests**: Have an idea? We'd love to hear it!
- ğŸ’» **Code Contributions**: Submit pull requests for fixes and features
- ğŸ“š **Documentation**: Help improve our docs and examples
- ğŸ§ª **Testing**: Help test new features and integrations

### Getting Started
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

PinData is open source software licensed under the [Apache License 2.0](LICENSE).

---

## ğŸŒŸ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=zutoData/pindata&type=Date)](https://star-history.com/#zutoData/pindata&Date)

---

![å¾®ä¿¡ç¾¤](./wechat_2025-06-10_175702_266.png)

<div align="center">

**Made with â¤ï¸ by the PinData Team**

[â­ Star us on GitHub](https://github.com/zutoData/pindata)

</div>
